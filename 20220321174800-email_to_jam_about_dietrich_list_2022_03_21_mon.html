<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>email to jam about dietrich-list [2022-03-21 Mon]</title>
<meta name="author" content="Nix build user" />
<meta name="generator" content="Org Mode" />
<style type="text/css">
  #content { max-width: 60em; margin: auto; }
  .title  { text-align: center;
             margin-bottom: .2em; }
  .subtitle { text-align: center;
              font-size: medium;
              font-weight: bold;
              margin-top:0; }
  .todo   { font-family: monospace; color: red; }
  .done   { font-family: monospace; color: green; }
  .priority { font-family: monospace; color: orange; }
  .tag    { background-color: #eee; font-family: monospace;
            padding: 2px; font-size: 80%; font-weight: normal; }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .org-right  { margin-left: auto; margin-right: 0px;  text-align: right; }
  .org-left   { margin-left: 0px;  margin-right: auto; text-align: left; }
  .org-center { margin-left: auto; margin-right: auto; text-align: center; }
  .underline { text-decoration: underline; }
  #postamble p, #preamble p { font-size: 90%; margin: .2em; }
  p.verse { margin-left: 3%; }
  pre {
    border: 1px solid #e6e6e6;
    border-radius: 3px;
    background-color: #f2f2f2;
    padding: 8pt;
    font-family: monospace;
    overflow: auto;
    margin: 1.2em;
  }
  pre.src {
    position: relative;
    overflow: auto;
  }
  pre.src:before {
    display: none;
    position: absolute;
    top: -8px;
    right: 12px;
    padding: 3px;
    color: #555;
    background-color: #f2f2f299;
  }
  pre.src:hover:before { display: inline; margin-top: 14px;}
  /* Languages per Org manual */
  pre.src-asymptote:before { content: 'Asymptote'; }
  pre.src-awk:before { content: 'Awk'; }
  pre.src-authinfo::before { content: 'Authinfo'; }
  pre.src-C:before { content: 'C'; }
  /* pre.src-C++ doesn't work in CSS */
  pre.src-clojure:before { content: 'Clojure'; }
  pre.src-css:before { content: 'CSS'; }
  pre.src-D:before { content: 'D'; }
  pre.src-ditaa:before { content: 'ditaa'; }
  pre.src-dot:before { content: 'Graphviz'; }
  pre.src-calc:before { content: 'Emacs Calc'; }
  pre.src-emacs-lisp:before { content: 'Emacs Lisp'; }
  pre.src-fortran:before { content: 'Fortran'; }
  pre.src-gnuplot:before { content: 'gnuplot'; }
  pre.src-haskell:before { content: 'Haskell'; }
  pre.src-hledger:before { content: 'hledger'; }
  pre.src-java:before { content: 'Java'; }
  pre.src-js:before { content: 'Javascript'; }
  pre.src-latex:before { content: 'LaTeX'; }
  pre.src-ledger:before { content: 'Ledger'; }
  pre.src-lisp:before { content: 'Lisp'; }
  pre.src-lilypond:before { content: 'Lilypond'; }
  pre.src-lua:before { content: 'Lua'; }
  pre.src-matlab:before { content: 'MATLAB'; }
  pre.src-mscgen:before { content: 'Mscgen'; }
  pre.src-ocaml:before { content: 'Objective Caml'; }
  pre.src-octave:before { content: 'Octave'; }
  pre.src-org:before { content: 'Org mode'; }
  pre.src-oz:before { content: 'OZ'; }
  pre.src-plantuml:before { content: 'Plantuml'; }
  pre.src-processing:before { content: 'Processing.js'; }
  pre.src-python:before { content: 'Python'; }
  pre.src-R:before { content: 'R'; }
  pre.src-ruby:before { content: 'Ruby'; }
  pre.src-sass:before { content: 'Sass'; }
  pre.src-scheme:before { content: 'Scheme'; }
  pre.src-screen:before { content: 'Gnu Screen'; }
  pre.src-sed:before { content: 'Sed'; }
  pre.src-sh:before { content: 'shell'; }
  pre.src-sql:before { content: 'SQL'; }
  pre.src-sqlite:before { content: 'SQLite'; }
  /* additional languages in org.el's org-babel-load-languages alist */
  pre.src-forth:before { content: 'Forth'; }
  pre.src-io:before { content: 'IO'; }
  pre.src-J:before { content: 'J'; }
  pre.src-makefile:before { content: 'Makefile'; }
  pre.src-maxima:before { content: 'Maxima'; }
  pre.src-perl:before { content: 'Perl'; }
  pre.src-picolisp:before { content: 'Pico Lisp'; }
  pre.src-scala:before { content: 'Scala'; }
  pre.src-shell:before { content: 'Shell Script'; }
  pre.src-ebnf2ps:before { content: 'ebfn2ps'; }
  /* additional language identifiers per "defun org-babel-execute"
       in ob-*.el */
  pre.src-cpp:before  { content: 'C++'; }
  pre.src-abc:before  { content: 'ABC'; }
  pre.src-coq:before  { content: 'Coq'; }
  pre.src-groovy:before  { content: 'Groovy'; }
  /* additional language identifiers from org-babel-shell-names in
     ob-shell.el: ob-shell is the only babel language using a lambda to put
     the execution function name together. */
  pre.src-bash:before  { content: 'bash'; }
  pre.src-csh:before  { content: 'csh'; }
  pre.src-ash:before  { content: 'ash'; }
  pre.src-dash:before  { content: 'dash'; }
  pre.src-ksh:before  { content: 'ksh'; }
  pre.src-mksh:before  { content: 'mksh'; }
  pre.src-posh:before  { content: 'posh'; }
  /* Additional Emacs modes also supported by the LaTeX listings package */
  pre.src-ada:before { content: 'Ada'; }
  pre.src-asm:before { content: 'Assembler'; }
  pre.src-caml:before { content: 'Caml'; }
  pre.src-delphi:before { content: 'Delphi'; }
  pre.src-html:before { content: 'HTML'; }
  pre.src-idl:before { content: 'IDL'; }
  pre.src-mercury:before { content: 'Mercury'; }
  pre.src-metapost:before { content: 'MetaPost'; }
  pre.src-modula-2:before { content: 'Modula-2'; }
  pre.src-pascal:before { content: 'Pascal'; }
  pre.src-ps:before { content: 'PostScript'; }
  pre.src-prolog:before { content: 'Prolog'; }
  pre.src-simula:before { content: 'Simula'; }
  pre.src-tcl:before { content: 'tcl'; }
  pre.src-tex:before { content: 'TeX'; }
  pre.src-plain-tex:before { content: 'Plain TeX'; }
  pre.src-verilog:before { content: 'Verilog'; }
  pre.src-vhdl:before { content: 'VHDL'; }
  pre.src-xml:before { content: 'XML'; }
  pre.src-nxml:before { content: 'XML'; }
  /* add a generic configuration mode; LaTeX export needs an additional
     (add-to-list 'org-latex-listings-langs '(conf " ")) in .emacs */
  pre.src-conf:before { content: 'Configuration File'; }

  table { border-collapse:collapse; }
  caption.t-above { caption-side: top; }
  caption.t-bottom { caption-side: bottom; }
  td, th { vertical-align:top;  }
  th.org-right  { text-align: center;  }
  th.org-left   { text-align: center;   }
  th.org-center { text-align: center; }
  td.org-right  { text-align: right;  }
  td.org-left   { text-align: left;   }
  td.org-center { text-align: center; }
  dt { font-weight: bold; }
  .footpara { display: inline; }
  .footdef  { margin-bottom: 1em; }
  .figure { padding: 1em; }
  .figure p { text-align: center; }
  .equation-container {
    display: table;
    text-align: center;
    width: 100%;
  }
  .equation {
    vertical-align: middle;
  }
  .equation-label {
    display: table-cell;
    text-align: right;
    vertical-align: middle;
  }
  .inlinetask {
    padding: 10px;
    border: 2px solid gray;
    margin: 10px;
    background: #ffffcc;
  }
  #org-div-home-and-up
   { text-align: right; font-size: 70%; white-space: nowrap; }
  textarea { overflow-x: auto; }
  .linenr { font-size: smaller }
  .code-highlighted { background-color: #ffff00; }
  .org-info-js_info-navigation { border-style: none; }
  #org-info-js_console-label
    { font-size: 10px; font-weight: bold; white-space: nowrap; }
  .org-info-js_search-highlight
    { background-color: #ffff00; color: #000000; font-weight: bold; }
  .org-svg { }
</style>
<link rel="stylesheet" type="text/css" href="https://gongzhitaao.org/orgcss/org.css"/>
<script>
  window.MathJax = {
    tex: {
      ams: {
        multlineWidth: '85%'
      },
      tags: 'ams',
      tagSide: 'right',
      tagIndent: '.8em'
    },
    chtml: {
      scale: 1.0,
      displayAlign: 'center',
      displayIndent: '0em'
    },
    svg: {
      scale: 1.0,
      displayAlign: 'center',
      displayIndent: '0em'
    },
    output: {
      font: 'mathjax-modern',
      displayOverflow: 'overflow'
    }
  };
</script>

<script
  id="MathJax-script"
  async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
</script>
</head>
<body>
<div id="content" class="content">
<h1 class="title">email to jam about dietrich-list <span class="timestamp-wrapper"><span class="timestamp">[2022-03-21 Mon]</span></span></h1>
<p>
Dear Jam,
</p>

<p>
I hope the past 20 days have been good to you. How've you been?
</p>

<p>
Vipassana went well, and they did indeed feed me enough. (At any rate it was enough not to feel hungry or faint. The day does involve a lot of sitting still.)
</p>

<p>
Here is what happened in the past week, and in part in my brain in the ten days preceding:
</p>
<p>
Dear Jam,
</p>

<p>
I hope the past 20 days have been good to you. How've you been?
</p>

<p>
Vipassana went well, and they did indeed feed me enough. (At any rate it was enough not to feel hungry or faint. The day does involve a lot of sitting still.)
</p>

<p>
Here is what happened in the past week, and in part in my brain in the ten days preceding:
</p>

<div id="outline-container-d351c9aa-09b7-4129-b161-1fc196fe2e39-misunderstanding-and-resolution" class="outline-2">
<h2 id="d351c9aa-09b7-4129-b161-1fc196fe2e39-misunderstanding-and-resolution"><span class="section-number-2">1.</span> misunderstanding and resolution</h2>
<div class="outline-text-2" id="text-d351c9aa-09b7-4129-b161-1fc196fe2e39-misunderstanding-and-resolution">
<ul class="org-ul">
<li>I realized I'd misundestood part of the Dietrich-List paper: I'd taken the alphabet of properties as constituting outcomes, but DL constructs them in the reverse way, saying that alternatives/outcomes constitute properties. This is much nicer for treating properties as a structure-preserving "intermediate representation" when going from motivational states to outcomes or vice versa.
<ul class="org-ul">
<li>It also solves "for free" the issue I was facing in building a player's preferences over alternatives from their preferences over properties: I was assuming that preferences over properties would be acyclic, but this is the exact assumption the paper is trying to dispense with :facepalm:</li>
</ul></li>
<li><p>
I reread the axiomatization to be sure I understood it correctly this time, and wrote a note and a distinguishing scenario:
</p>

<p>
The model discussed by this paper is one where the outcomes of a game each have various properties; and player preferences operate over these properties, rather than the outcomes themselves.
</p>

<p>
In addition, players filter their preferences on the basis of which properties they may consider relevant. That set of relevant properties is called a player's motivational state. Each player has a set of possible motivational states.
</p>

<p>
Two axiom systems are presented: 1+2, and 1+3.
</p>
<ol class="org-ol">
<li>motivational states that contain no properties that discriminate between two outcomes do not generate preferences that distinguish between the outcomes.</li>
<li>for motivational states \(A,B, A \subset B\), if \(B/A\) contains no properties that are present in two outcomes, then \(B\) and \(A\) agree with respect to those outcomes.</li>
<li>for motivational states \(A,B, A \subset B\), if \(B/A\) contains no properties that discriminate between two outcomes, then \(B\) and \(A\) agree with respect to those outcomes.</li>
</ol>

<p>
A scenario that distinguishes the systems:
</p>

<p>
Suppose motivational states \(A \subset B \subset C\); properties \(p_1 \in A, p_2 \in B/A, p_3 \in C/B\); outcomes \(x \in p_1, y \notin p_1, x,y \in p_2, x \in p_3, y \notin p_3\).
</p>

<ul class="org-ul">
<li>Under 1+2 system, there can exist a player preference family such that \(x \preceq_{A} y \land y \preceq_{B} x\).</li>
<li>Under 1+3 system this would not be possible; \(p_2\) contains both outcomes, which mean it does not distinguish between them. But \(x \preceq_{B} y, y \preceq_{C} x\) is possible, because \(p_3\) <i>does</i> distinguish between the outcomes.</li>
</ul>

<p>
Two theorems are presented:
</p>

<ol class="org-ol">
<li>If \(\mathcal{M}\) is intersection-closed, an agent's family of preference-orders satisfies 1+2 iff it is "property-based".</li>
<li>If \(\mathcal{M}\) is subset-closed, an agent's family of preference-orders satisfies 1+2 iff it is "property-based in a separable way", i.e. the <i>ranking over the powerset of all properties</i> exhibits independence of irrelevant alternatives. \(S_1 \ge S_2 \text{ iff } S_1 \cup T \ge S_2\cup T\)</li>
</ol></li>
</ul>
</div>
</div>
<div class="outline-text-2" id="text-d351c9aa-09b7-4129-b161-1fc196fe2e39-misunderstanding-and-resolution">
<ul class="org-ul">
<li>I realized I'd misundestood part of the Dietrich-List paper: I'd taken the alphabet of properties as constituting outcomes, but DL constructs them in the reverse way, saying that alternatives/outcomes constitute properties. This is much nicer for treating properties as a structure-preserving "intermediate representation" when going from motivational states to outcomes or vice versa.
<ul class="org-ul">
<li>It also solves "for free" the issue I was facing in building a player's preferences over alternatives from their preferences over properties: I was assuming that preferences over properties would be acyclic, but this is the exact assumption the paper is trying to dispense with :facepalm:</li>
</ul></li>
<li><p>
I reread the axiomatization to be sure I understood it correctly this time, and wrote a note and a distinguishing scenario:
</p>

<p>
The model discussed by this paper is one where the outcomes of a game each have various properties; and player preferences operate over these properties, rather than the outcomes themselves.
</p>

<p>
In addition, players filter their preferences on the basis of which properties they may consider relevant. That set of relevant properties is called a player's motivational state. Each player has a set of possible motivational states.
</p>

<p>
Two axiom systems are presented: 1+2, and 1+3.
</p>
<ol class="org-ol">
<li>motivational states that contain no properties that discriminate between two outcomes do not generate preferences that distinguish between the outcomes.</li>
<li>for motivational states \(A,B, A \subset B\), if \(B/A\) contains no properties that are present in two outcomes, then \(B\) and \(A\) agree with respect to those outcomes.</li>
<li>for motivational states \(A,B, A \subset B\), if \(B/A\) contains no properties that discriminate between two outcomes, then \(B\) and \(A\) agree with respect to those outcomes.</li>
</ol>

<p>
A scenario that distinguishes the systems:
</p>

<p>
Suppose motivational states \(A \subset B \subset C\); properties \(p_1 \in A, p_2 \in B/A, p_3 \in C/B\); outcomes \(x \in p_1, y \notin p_1, x,y \in p_2, x \in p_3, y \notin p_3\).
</p>

<ul class="org-ul">
<li>Under 1+2 system, there can exist a player preference family such that \(x \preceq_{A} y \land y \preceq_{B} x\).</li>
<li>Under 1+3 system this would not be possible; \(p_2\) contains both outcomes, which mean it does not distinguish between them. But \(x \preceq_{B} y, y \preceq_{C} x\) is possible, because \(p_3\) <i>does</i> distinguish between the outcomes.</li>
</ul>

<p>
Two theorems are presented:
</p>

<ol class="org-ol">
<li>If \(\mathcal{M}\) is intersection-closed, an agent's family of preference-orders satisfies 1+2 iff it is "property-based".</li>
<li>If \(\mathcal{M}\) is subset-closed, an agent's family of preference-orders satisfies 1+2 iff it is "property-based in a separable way", i.e. the <i>ranking over the powerset of all properties</i> exhibits independence of irrelevant alternatives. \(S_1 \ge S_2 \text{ iff } S_1 \cup T \ge S_2\cup T\)</li>
</ol></li>
</ul>
</div>
<ul class="org-ul">
<li>I realized I'd misundestood part of the Dietrich-List paper: I'd taken the alphabet of properties as constituting outcomes, but DL constructs them in the reverse way, saying that alternatives/outcomes constitute properties. This is much nicer for treating properties as a structure-preserving "intermediate representation" when going from motivational states to outcomes or vice versa.
<ul class="org-ul">
<li>It also solves "for free" the issue I was facing in building a player's preferences over alternatives from their preferences over properties: I was assuming that preferences over properties would be acyclic, but this is the exact assumption the paper is trying to dispense with :facepalm:</li>
</ul></li>
<li><p>
I reread the axiomatization to be sure I understood it correctly this time, and wrote a note and a distinguishing scenario:
</p>

<p>
The model discussed by this paper is one where the outcomes of a game each have various properties; and player preferences operate over these properties, rather than the outcomes themselves.
</p>

<p>
In addition, players filter their preferences on the basis of which properties they may consider relevant. That set of relevant properties is called a player's motivational state. Each player has a set of possible motivational states.
</p>

<p>
Two axiom systems are presented: 1+2, and 1+3.
</p>
<ol class="org-ol">
<li>motivational states that contain no properties that discriminate between two outcomes do not generate preferences that distinguish between the outcomes.</li>
<li>for motivational states \(A,B, A \subset B\), if \(B/A\) contains no properties that are present in two outcomes, then \(B\) and \(A\) agree with respect to those outcomes.</li>
<li>for motivational states \(A,B, A \subset B\), if \(B/A\) contains no properties that discriminate between two outcomes, then \(B\) and \(A\) agree with respect to those outcomes.</li>
</ol>

<p>
A scenario that distinguishes the systems:
</p>

<p>
Suppose motivational states \(A \subset B \subset C\); properties \(p_1 \in A, p_2 \in B/A, p_3 \in C/B\); outcomes \(x \in p_1, y \notin p_1, x,y \in p_2, x \in p_3, y \notin p_3\).
</p>

<ul class="org-ul">
<li>Under 1+2 system, there can exist a player preference family such that \(x \preceq_{A} y \land y \preceq_{B} x\).</li>
<li>Under 1+3 system this would not be possible; \(p_2\) contains both outcomes, which mean it does not distinguish between them. But \(x \preceq_{B} y, y \preceq_{C} x\) is possible, because \(p_3\) <i>does</i> distinguish between the outcomes.</li>
</ul>

<p>
Two theorems are presented:
</p>

<ol class="org-ol">
<li>If \(\mathcal{M}\) is intersection-closed, an agent's family of preference-orders satisfies 1+2 iff it is "property-based".</li>
<li>If \(\mathcal{M}\) is subset-closed, an agent's family of preference-orders satisfies 1+2 iff it is "property-based in a separable way", i.e. the <i>ranking over the powerset of all properties</i> exhibits independence of irrelevant alternatives. \(S_1 \ge S_2 \text{ iff } S_1 \cup T \ge S_2\cup T\)</li>
</ol></li>
</ul>

<li>I realized I'd misundestood part of the Dietrich-List paper: I'd taken the alphabet of properties as constituting outcomes, but DL constructs them in the reverse way, saying that alternatives/outcomes constitute properties. This is much nicer for treating properties as a structure-preserving "intermediate representation" when going from motivational states to outcomes or vice versa.
<ul class="org-ul">
<li>It also solves "for free" the issue I was facing in building a player's preferences over alternatives from their preferences over properties: I was assuming that preferences over properties would be acyclic, but this is the exact assumption the paper is trying to dispense with :facepalm:</li>
</ul></li>
I realized I'd misundestood part of the Dietrich-List paper: I'd taken the alphabet of properties as constituting outcomes, but DL constructs them in the reverse way, saying that alternatives/outcomes constitute properties. This is much nicer for treating properties as a structure-preserving "intermediate representation" when going from motivational states to outcomes or vice versa.
<ul class="org-ul">
<li>It also solves "for free" the issue I was facing in building a player's preferences over alternatives from their preferences over properties: I was assuming that preferences over properties would be acyclic, but this is the exact assumption the paper is trying to dispense with :facepalm:</li>
</ul>
<li>It also solves "for free" the issue I was facing in building a player's preferences over alternatives from their preferences over properties: I was assuming that preferences over properties would be acyclic, but this is the exact assumption the paper is trying to dispense with :facepalm:</li>
It also solves "for free" the issue I was facing in building a player's preferences over alternatives from their preferences over properties: I was assuming that preferences over properties would be acyclic, but this is the exact assumption the paper is trying to dispense with :facepalm:
<li><p>
I reread the axiomatization to be sure I understood it correctly this time, and wrote a note and a distinguishing scenario:
</p>

<p>
The model discussed by this paper is one where the outcomes of a game each have various properties; and player preferences operate over these properties, rather than the outcomes themselves.
</p>

<p>
In addition, players filter their preferences on the basis of which properties they may consider relevant. That set of relevant properties is called a player's motivational state. Each player has a set of possible motivational states.
</p>

<p>
Two axiom systems are presented: 1+2, and 1+3.
</p>
<ol class="org-ol">
<li>motivational states that contain no properties that discriminate between two outcomes do not generate preferences that distinguish between the outcomes.</li>
<li>for motivational states \(A,B, A \subset B\), if \(B/A\) contains no properties that are present in two outcomes, then \(B\) and \(A\) agree with respect to those outcomes.</li>
<li>for motivational states \(A,B, A \subset B\), if \(B/A\) contains no properties that discriminate between two outcomes, then \(B\) and \(A\) agree with respect to those outcomes.</li>
</ol>

<p>
A scenario that distinguishes the systems:
</p>

<p>
Suppose motivational states \(A \subset B \subset C\); properties \(p_1 \in A, p_2 \in B/A, p_3 \in C/B\); outcomes \(x \in p_1, y \notin p_1, x,y \in p_2, x \in p_3, y \notin p_3\).
</p>

<ul class="org-ul">
<li>Under 1+2 system, there can exist a player preference family such that \(x \preceq_{A} y \land y \preceq_{B} x\).</li>
<li>Under 1+3 system this would not be possible; \(p_2\) contains both outcomes, which mean it does not distinguish between them. But \(x \preceq_{B} y, y \preceq_{C} x\) is possible, because \(p_3\) <i>does</i> distinguish between the outcomes.</li>
</ul>

<p>
Two theorems are presented:
</p>

<ol class="org-ol">
<li>If \(\mathcal{M}\) is intersection-closed, an agent's family of preference-orders satisfies 1+2 iff it is "property-based".</li>
<li>If \(\mathcal{M}\) is subset-closed, an agent's family of preference-orders satisfies 1+2 iff it is "property-based in a separable way", i.e. the <i>ranking over the powerset of all properties</i> exhibits independence of irrelevant alternatives. \(S_1 \ge S_2 \text{ iff } S_1 \cup T \ge S_2\cup T\)</li>
</ol></li>
<p>
I reread the axiomatization to be sure I understood it correctly this time, and wrote a note and a distinguishing scenario:
</p>

<p>
The model discussed by this paper is one where the outcomes of a game each have various properties; and player preferences operate over these properties, rather than the outcomes themselves.
</p>

<p>
In addition, players filter their preferences on the basis of which properties they may consider relevant. That set of relevant properties is called a player's motivational state. Each player has a set of possible motivational states.
</p>

<p>
Two axiom systems are presented: 1+2, and 1+3.
</p>
<ol class="org-ol">
<li>motivational states that contain no properties that discriminate between two outcomes do not generate preferences that distinguish between the outcomes.</li>
<li>for motivational states \(A,B, A \subset B\), if \(B/A\) contains no properties that are present in two outcomes, then \(B\) and \(A\) agree with respect to those outcomes.</li>
<li>for motivational states \(A,B, A \subset B\), if \(B/A\) contains no properties that discriminate between two outcomes, then \(B\) and \(A\) agree with respect to those outcomes.</li>
</ol>

<li>motivational states that contain no properties that discriminate between two outcomes do not generate preferences that distinguish between the outcomes.</li>
motivational states that contain no properties that discriminate between two outcomes do not generate preferences that distinguish between the outcomes.
<li>for motivational states \(A,B, A \subset B\), if \(B/A\) contains no properties that are present in two outcomes, then \(B\) and \(A\) agree with respect to those outcomes.</li>
for motivational states \(A,B, A \subset B\), if \(B/A\) contains no properties that are present in two outcomes, then \(B\) and \(A\) agree with respect to those outcomes.
<li>for motivational states \(A,B, A \subset B\), if \(B/A\) contains no properties that discriminate between two outcomes, then \(B\) and \(A\) agree with respect to those outcomes.</li>
for motivational states \(A,B, A \subset B\), if \(B/A\) contains no properties that discriminate between two outcomes, then \(B\) and \(A\) agree with respect to those outcomes.
<p>
A scenario that distinguishes the systems:
</p>

<p>
Suppose motivational states \(A \subset B \subset C\); properties \(p_1 \in A, p_2 \in B/A, p_3 \in C/B\); outcomes \(x \in p_1, y \notin p_1, x,y \in p_2, x \in p_3, y \notin p_3\).
</p>

<ul class="org-ul">
<li>Under 1+2 system, there can exist a player preference family such that \(x \preceq_{A} y \land y \preceq_{B} x\).</li>
<li>Under 1+3 system this would not be possible; \(p_2\) contains both outcomes, which mean it does not distinguish between them. But \(x \preceq_{B} y, y \preceq_{C} x\) is possible, because \(p_3\) <i>does</i> distinguish between the outcomes.</li>
</ul>

<li>Under 1+2 system, there can exist a player preference family such that \(x \preceq_{A} y \land y \preceq_{B} x\).</li>
Under 1+2 system, there can exist a player preference family such that \(x \preceq_{A} y \land y \preceq_{B} x\).
<li>Under 1+3 system this would not be possible; \(p_2\) contains both outcomes, which mean it does not distinguish between them. But \(x \preceq_{B} y, y \preceq_{C} x\) is possible, because \(p_3\) <i>does</i> distinguish between the outcomes.</li>
Under 1+3 system this would not be possible; \(p_2\) contains both outcomes, which mean it does not distinguish between them. But \(x \preceq_{B} y, y \preceq_{C} x\) is possible, because \(p_3\) <i>does</i> distinguish between the outcomes.
<p>
Two theorems are presented:
</p>

<ol class="org-ol">
<li>If \(\mathcal{M}\) is intersection-closed, an agent's family of preference-orders satisfies 1+2 iff it is "property-based".</li>
<li>If \(\mathcal{M}\) is subset-closed, an agent's family of preference-orders satisfies 1+2 iff it is "property-based in a separable way", i.e. the <i>ranking over the powerset of all properties</i> exhibits independence of irrelevant alternatives. \(S_1 \ge S_2 \text{ iff } S_1 \cup T \ge S_2\cup T\)</li>
</ol>
<li>If \(\mathcal{M}\) is intersection-closed, an agent's family of preference-orders satisfies 1+2 iff it is "property-based".</li>
If \(\mathcal{M}\) is intersection-closed, an agent's family of preference-orders satisfies 1+2 iff it is "property-based".
<li>If \(\mathcal{M}\) is subset-closed, an agent's family of preference-orders satisfies 1+2 iff it is "property-based in a separable way", i.e. the <i>ranking over the powerset of all properties</i> exhibits independence of irrelevant alternatives. \(S_1 \ge S_2 \text{ iff } S_1 \cup T \ge S_2\cup T\)</li>
If \(\mathcal{M}\) is subset-closed, an agent's family of preference-orders satisfies 1+2 iff it is "property-based in a separable way", i.e. the <i>ranking over the powerset of all properties</i> exhibits independence of irrelevant alternatives. \(S_1 \ge S_2 \text{ iff } S_1 \cup T \ge S_2\cup T\)
<div id="outline-container-d351c9aa-09b7-4129-b161-1fc196fe2e39-a-topology-over-properties" class="outline-2">
<h2 id="d351c9aa-09b7-4129-b161-1fc196fe2e39-a-topology-over-properties"><span class="section-number-2">2.</span> A topology over properties</h2>
<div class="outline-text-2" id="text-d351c9aa-09b7-4129-b161-1fc196fe2e39-a-topology-over-properties">
<p>
We have been discussing making the set \(\mathcal{M}\) of a player's possible motivational states a topology, in order to use it as an "organizing principle" that lets us start engaging with attaching meanings to the properties that have some internal structure. I think I understand some of <i>why</i> now: a topology over properties might let us treat the properties as sentences in a language. The semantics of the language ought to follow from its motivating example, and then dictate what the structure even ought to be; but its at least allowing unions and intersections, and having a Top and a Bottom, seem like good starting assumptions.
</p>

<p>
\(mathcal{M}\) being a topology is ensured by axiom 3, and under axiom 2 it's actually a pi-system? Apparently? Which is to say, Wikipedia tells me that this is the name for a  subset of a powerset that is intersection-closed but not union-closed.
</p>

<p>
The function to output the topological closure (is this valid phrasing?) of a given subset of a powerset is written. What relies on it holding? I remain unsure, and am going to try to approach the problem from the angle of the motivating examples instead of tooling around here without, well, motivation.
</p>
</div>
</div>
<div class="outline-text-2" id="text-d351c9aa-09b7-4129-b161-1fc196fe2e39-a-topology-over-properties">
<p>
We have been discussing making the set \(\mathcal{M}\) of a player's possible motivational states a topology, in order to use it as an "organizing principle" that lets us start engaging with attaching meanings to the properties that have some internal structure. I think I understand some of <i>why</i> now: a topology over properties might let us treat the properties as sentences in a language. The semantics of the language ought to follow from its motivating example, and then dictate what the structure even ought to be; but its at least allowing unions and intersections, and having a Top and a Bottom, seem like good starting assumptions.
</p>

<p>
\(mathcal{M}\) being a topology is ensured by axiom 3, and under axiom 2 it's actually a pi-system? Apparently? Which is to say, Wikipedia tells me that this is the name for a  subset of a powerset that is intersection-closed but not union-closed.
</p>

<p>
The function to output the topological closure (is this valid phrasing?) of a given subset of a powerset is written. What relies on it holding? I remain unsure, and am going to try to approach the problem from the angle of the motivating examples instead of tooling around here without, well, motivation.
</p>
</div>
<p>
We have been discussing making the set \(\mathcal{M}\) of a player's possible motivational states a topology, in order to use it as an "organizing principle" that lets us start engaging with attaching meanings to the properties that have some internal structure. I think I understand some of <i>why</i> now: a topology over properties might let us treat the properties as sentences in a language. The semantics of the language ought to follow from its motivating example, and then dictate what the structure even ought to be; but its at least allowing unions and intersections, and having a Top and a Bottom, seem like good starting assumptions.
</p>

<p>
\(mathcal{M}\) being a topology is ensured by axiom 3, and under axiom 2 it's actually a pi-system? Apparently? Which is to say, Wikipedia tells me that this is the name for a  subset of a powerset that is intersection-closed but not union-closed.
</p>

<p>
The function to output the topological closure (is this valid phrasing?) of a given subset of a powerset is written. What relies on it holding? I remain unsure, and am going to try to approach the problem from the angle of the motivating examples instead of tooling around here without, well, motivation.
</p>

<div id="outline-container-d351c9aa-09b7-4129-b161-1fc196fe2e39-ideas-for-semantics" class="outline-2">
<h2 id="d351c9aa-09b7-4129-b161-1fc196fe2e39-ideas-for-semantics"><span class="section-number-2">3.</span> Ideas for semantics</h2>
<div class="outline-text-2" id="text-d351c9aa-09b7-4129-b161-1fc196fe2e39-ideas-for-semantics">
<p>
Some ideas for the semantics we can assign to this system:
</p>
</div>
<div id="outline-container-d351c9aa-09b7-4129-b161-1fc196fe2e39-properties-of-outcomes-as-intentions" class="outline-3">
<h3 id="d351c9aa-09b7-4129-b161-1fc196fe2e39-properties-of-outcomes-as-intentions"><span class="section-number-3">3.1.</span> Properties of outcomes as intentions</h3>
<div class="outline-text-3" id="text-d351c9aa-09b7-4129-b161-1fc196fe2e39-properties-of-outcomes-as-intentions">
<p>
I've been reading G.E.M. Anscombe's <i>Intention</i>, and thoguht of this.
</p>

<p>
Agents select for outcomes, operating from inside certain contexts. Properties that are derivable by applying DL's theorems can then be a construction of "what the agent was going for."
</p>

<p>
What is the organizing principle of the properties that then tells us the relationship between aim A and aim B? That structure is then the structure of intention.
</p>

<p>
We can take Anscombe as a first step to understanding how to assign semantics to motivational states, outcomes, and properties respectively; and what structure over properties to begin looking at (and what might follow as a result.)
</p>

<p>
Components of organizing principles that suggest themselves, from our understanding so far:
</p>
<ul class="org-ul">
<li>epistemics. the motivational states are indications of what agents know about the alternatives. thus, the properties are factual assertions about the outcomes that matter to agents.</li>
<li>the properties of outcomes can be factual assertions that describe the extensional game further
<ul class="org-ul">
<li>assertions about reachability.</li>
<li>assertions stronger than reachability</li>
<li>conditions that help us answer the question "given what they did, what did they want?" ("can we derive it? can we prove it?")</li>
<li>conditions that help us assert (or, less likely, disprove) that "given that an agent did that, they must have wanted <i>something</i> that is congruent to the structure presented." This gives us a falsifiable assertion that I can throw a dataset at.</li>
</ul></li>
<li>relevance. the motivational states represent the playing-out of a resource bound on reasoning, which we can understand to be true based on how many properties are in each state - which is monotonic to how many outcomes the agent can distinguish between.</li>
</ul>


<p>
I will send my summary notes on Anscombe once I have made a fair copy. Their current form is, in several senses, unreadable chicken scratch.
</p>
</div>
</div>
<div id="outline-container-d351c9aa-09b7-4129-b161-1fc196fe2e39-reasons-why-players-might-move" class="outline-3">
<h3 id="d351c9aa-09b7-4129-b161-1fc196fe2e39-reasons-why-players-might-move"><span class="section-number-3">3.2.</span> reasons why players might move from one motivational state to the other:</h3>
<div class="outline-text-3" id="text-d351c9aa-09b7-4129-b161-1fc196fe2e39-reasons-why-players-might-move">
<p>
The condition kept in mind while generating these: the reasoning must be expressed in terms already defined in the game structure, or derived from them.
</p>

<ul class="org-ul">
<li>The motivstates represent what players know about the outcomes - updating to add a property is adding a fact to the universe of consideration.</li>
<li>The motivstates represent "relevance" - updating to add a property necessitates dropping another one, and players optimize for reaching outcomes that would be preferred under a maximal motivstate that they can't actually ever hold.</li>
<li>The motivstates represent information about the game itself:
<ul class="org-ul">
<li>Reachability - it would be a simplification of the game tree</li>
<li>Whether some future game is gluable onto an outcome.</li>
<li>A fact about <i>the other players</i> (what type would that fact have?)
<ul class="org-ul">
<li>We are trying to assert that motivstates are things that players can infer about other players. What would they be trying to find out?
<ul class="org-ul">
<li>Easy answer: what the other players are going to do. So, a partial strategy? Something of the form "\(p \in M_{Alice}\) iff Alice expects Bob to move left at some point in future play".</li>
<li>Another easy answer: what players want. So: "\(f(p) \in M_{Alice}\) iff Alice thinks Bob is ranking some property \(p\) higher than all other properties."</li>
</ul></li>
</ul></li>
<li>Verbs - intentions that connect moves to ends, that can be inferred and matched to signalling moves over the course of play.</li>
</ul></li>
<li>The motivstates represent concerns that are apt to some environmental fact: e.g. the season dictating the parameters for selecting fruit. In monsoon you must look for thick skins, in summer you must look for high water content, etc. Interesting, because it's a way to show that the preference cycle might be entrained by an environmental cycle, and therefore rational in context.
<ul class="org-ul">
<li>This is called a zeitgeber. <a href="https://astralcodexten.substack.com/p/diseasonality?s=r">Diseasonality - by Scott Alexander - Astral Codex Ten</a> is where I first head about this; there's a LARGE body of literature attached to the idea of a zeitgeber, and I should read it if and when it seems like a good idea to. Modeling it using a similar kind of intransitivity to what we're playing with here would be pretty cool.
<ul class="org-ul">
<li>Spitball intuition: size of motivstate tracks how "tight" your curves can be on a cycle - shifting sands do not make for fine-grained preferences.
<ul class="org-ul">
<li>something something lotka-volterra
<ul class="org-ul">
<li>I do need to spend some time studying this magic when I can.</li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
</ul>
</div>
<div id="outline-container-d351c9aa-09b7-4129-b161-1fc196fe2e39-concrete-question-let-properties-be" class="outline-4">
<h4 id="d351c9aa-09b7-4129-b161-1fc196fe2e39-concrete-question-let-properties-be"><span class="section-number-4">3.2.1.</span> CONCRETE QUESTION: let properties be words in the builder-assistant game. How many rounds of play does it take before the epistemic game catches up to common knowledge of which properties constitute a player p's motivational state?</h4>
<div class="outline-text-4" id="text-d351c9aa-09b7-4129-b161-1fc196fe2e39-concrete-question-let-properties-be">
<p>
Let there exist a turn based two-player game.
</p>
<ul class="org-ul">
<li>We begin at state R
<ul class="org-ul">
<li>At R, A, and B, one can either</li>
<li>play a, which takes one to state A</li>
<li>play b, which takes one to state B</li>
<li>play s, which takes one to state S, where both players must play either x or y
<ul class="org-ul">
<li>if both players select x or both select y, we go to state WIN; each get one point.</li>
<li>else we go back to R.</li>
</ul></li>
</ul></li>
</ul>


<p>
This can be extended over some arbitrary alphabet of states-and-moves. Call that alphabet the builder-assistant language.
</p>

<p>
I think there exist property-based agents who have a winning strategy at this game.
</p>

<p>
Once we have the collaborative picture, then we can try to break it.
</p>


<ul class="org-ul">
<li>can we build a simulation in which siloing happens?
<ul class="org-ul">
<li>I'm defining "siloing" as "several coalitions achieving coordination internally takes fewer steps to reach than the grand coalition achieving coordination."</li>
</ul></li>
<li>can we define a complexity measure according to which a population speaking a language exhibits siloing, or other such effects, past a threshold in that measure?
<ul class="org-ul">
<li>My naive first guess for a complexity measure is "number of tokens in the motivstate."</li>
<li>My second guess is "minimum number of steps needed to achieve common knowledge of everybody's motivstate in a coalition."</li>
</ul></li>
<li>what is the "shape" of the game that exhibits the minimum number of steps needed to achieve common knowledge?</li>
</ul>



<p>
If I recall correctly you have mentioned you and Parkih 2004 as a referent for agents agreeing upon a protocol of further discourse. I will go read that this week.
</p>
</div>
</div>
</div>
</div>
<div class="outline-text-2" id="text-d351c9aa-09b7-4129-b161-1fc196fe2e39-ideas-for-semantics">
<p>
Some ideas for the semantics we can assign to this system:
</p>
</div>
<p>
Some ideas for the semantics we can assign to this system:
</p>

<div id="outline-container-d351c9aa-09b7-4129-b161-1fc196fe2e39-properties-of-outcomes-as-intentions" class="outline-3">
<h3 id="d351c9aa-09b7-4129-b161-1fc196fe2e39-properties-of-outcomes-as-intentions"><span class="section-number-3">3.1.</span> Properties of outcomes as intentions</h3>
<div class="outline-text-3" id="text-d351c9aa-09b7-4129-b161-1fc196fe2e39-properties-of-outcomes-as-intentions">
<p>
I've been reading G.E.M. Anscombe's <i>Intention</i>, and thoguht of this.
</p>

<p>
Agents select for outcomes, operating from inside certain contexts. Properties that are derivable by applying DL's theorems can then be a construction of "what the agent was going for."
</p>

<p>
What is the organizing principle of the properties that then tells us the relationship between aim A and aim B? That structure is then the structure of intention.
</p>

<p>
We can take Anscombe as a first step to understanding how to assign semantics to motivational states, outcomes, and properties respectively; and what structure over properties to begin looking at (and what might follow as a result.)
</p>

<p>
Components of organizing principles that suggest themselves, from our understanding so far:
</p>
<ul class="org-ul">
<li>epistemics. the motivational states are indications of what agents know about the alternatives. thus, the properties are factual assertions about the outcomes that matter to agents.</li>
<li>the properties of outcomes can be factual assertions that describe the extensional game further
<ul class="org-ul">
<li>assertions about reachability.</li>
<li>assertions stronger than reachability</li>
<li>conditions that help us answer the question "given what they did, what did they want?" ("can we derive it? can we prove it?")</li>
<li>conditions that help us assert (or, less likely, disprove) that "given that an agent did that, they must have wanted <i>something</i> that is congruent to the structure presented." This gives us a falsifiable assertion that I can throw a dataset at.</li>
</ul></li>
<li>relevance. the motivational states represent the playing-out of a resource bound on reasoning, which we can understand to be true based on how many properties are in each state - which is monotonic to how many outcomes the agent can distinguish between.</li>
</ul>


<p>
I will send my summary notes on Anscombe once I have made a fair copy. Their current form is, in several senses, unreadable chicken scratch.
</p>
</div>
</div>
<div class="outline-text-3" id="text-d351c9aa-09b7-4129-b161-1fc196fe2e39-properties-of-outcomes-as-intentions">
<p>
I've been reading G.E.M. Anscombe's <i>Intention</i>, and thoguht of this.
</p>

<p>
Agents select for outcomes, operating from inside certain contexts. Properties that are derivable by applying DL's theorems can then be a construction of "what the agent was going for."
</p>

<p>
What is the organizing principle of the properties that then tells us the relationship between aim A and aim B? That structure is then the structure of intention.
</p>

<p>
We can take Anscombe as a first step to understanding how to assign semantics to motivational states, outcomes, and properties respectively; and what structure over properties to begin looking at (and what might follow as a result.)
</p>

<p>
Components of organizing principles that suggest themselves, from our understanding so far:
</p>
<ul class="org-ul">
<li>epistemics. the motivational states are indications of what agents know about the alternatives. thus, the properties are factual assertions about the outcomes that matter to agents.</li>
<li>the properties of outcomes can be factual assertions that describe the extensional game further
<ul class="org-ul">
<li>assertions about reachability.</li>
<li>assertions stronger than reachability</li>
<li>conditions that help us answer the question "given what they did, what did they want?" ("can we derive it? can we prove it?")</li>
<li>conditions that help us assert (or, less likely, disprove) that "given that an agent did that, they must have wanted <i>something</i> that is congruent to the structure presented." This gives us a falsifiable assertion that I can throw a dataset at.</li>
</ul></li>
<li>relevance. the motivational states represent the playing-out of a resource bound on reasoning, which we can understand to be true based on how many properties are in each state - which is monotonic to how many outcomes the agent can distinguish between.</li>
</ul>


<p>
I will send my summary notes on Anscombe once I have made a fair copy. Their current form is, in several senses, unreadable chicken scratch.
</p>
</div>
<p>
I've been reading G.E.M. Anscombe's <i>Intention</i>, and thoguht of this.
</p>

<p>
Agents select for outcomes, operating from inside certain contexts. Properties that are derivable by applying DL's theorems can then be a construction of "what the agent was going for."
</p>

<p>
What is the organizing principle of the properties that then tells us the relationship between aim A and aim B? That structure is then the structure of intention.
</p>

<p>
We can take Anscombe as a first step to understanding how to assign semantics to motivational states, outcomes, and properties respectively; and what structure over properties to begin looking at (and what might follow as a result.)
</p>

<p>
Components of organizing principles that suggest themselves, from our understanding so far:
</p>
<ul class="org-ul">
<li>epistemics. the motivational states are indications of what agents know about the alternatives. thus, the properties are factual assertions about the outcomes that matter to agents.</li>
<li>the properties of outcomes can be factual assertions that describe the extensional game further
<ul class="org-ul">
<li>assertions about reachability.</li>
<li>assertions stronger than reachability</li>
<li>conditions that help us answer the question "given what they did, what did they want?" ("can we derive it? can we prove it?")</li>
<li>conditions that help us assert (or, less likely, disprove) that "given that an agent did that, they must have wanted <i>something</i> that is congruent to the structure presented." This gives us a falsifiable assertion that I can throw a dataset at.</li>
</ul></li>
<li>relevance. the motivational states represent the playing-out of a resource bound on reasoning, which we can understand to be true based on how many properties are in each state - which is monotonic to how many outcomes the agent can distinguish between.</li>
</ul>


<li>epistemics. the motivational states are indications of what agents know about the alternatives. thus, the properties are factual assertions about the outcomes that matter to agents.</li>
epistemics. the motivational states are indications of what agents know about the alternatives. thus, the properties are factual assertions about the outcomes that matter to agents.
<li>the properties of outcomes can be factual assertions that describe the extensional game further
<ul class="org-ul">
<li>assertions about reachability.</li>
<li>assertions stronger than reachability</li>
<li>conditions that help us answer the question "given what they did, what did they want?" ("can we derive it? can we prove it?")</li>
<li>conditions that help us assert (or, less likely, disprove) that "given that an agent did that, they must have wanted <i>something</i> that is congruent to the structure presented." This gives us a falsifiable assertion that I can throw a dataset at.</li>
</ul></li>
the properties of outcomes can be factual assertions that describe the extensional game further
<ul class="org-ul">
<li>assertions about reachability.</li>
<li>assertions stronger than reachability</li>
<li>conditions that help us answer the question "given what they did, what did they want?" ("can we derive it? can we prove it?")</li>
<li>conditions that help us assert (or, less likely, disprove) that "given that an agent did that, they must have wanted <i>something</i> that is congruent to the structure presented." This gives us a falsifiable assertion that I can throw a dataset at.</li>
</ul>
<li>assertions about reachability.</li>
assertions about reachability.
<li>assertions stronger than reachability</li>
assertions stronger than reachability
<li>conditions that help us answer the question "given what they did, what did they want?" ("can we derive it? can we prove it?")</li>
conditions that help us answer the question "given what they did, what did they want?" ("can we derive it? can we prove it?")
<li>conditions that help us assert (or, less likely, disprove) that "given that an agent did that, they must have wanted <i>something</i> that is congruent to the structure presented." This gives us a falsifiable assertion that I can throw a dataset at.</li>
conditions that help us assert (or, less likely, disprove) that "given that an agent did that, they must have wanted <i>something</i> that is congruent to the structure presented." This gives us a falsifiable assertion that I can throw a dataset at.
<li>relevance. the motivational states represent the playing-out of a resource bound on reasoning, which we can understand to be true based on how many properties are in each state - which is monotonic to how many outcomes the agent can distinguish between.</li>
relevance. the motivational states represent the playing-out of a resource bound on reasoning, which we can understand to be true based on how many properties are in each state - which is monotonic to how many outcomes the agent can distinguish between.
<p>
I will send my summary notes on Anscombe once I have made a fair copy. Their current form is, in several senses, unreadable chicken scratch.
</p>

<div id="outline-container-d351c9aa-09b7-4129-b161-1fc196fe2e39-reasons-why-players-might-move" class="outline-3">
<h3 id="d351c9aa-09b7-4129-b161-1fc196fe2e39-reasons-why-players-might-move"><span class="section-number-3">3.2.</span> reasons why players might move from one motivational state to the other:</h3>
<div class="outline-text-3" id="text-d351c9aa-09b7-4129-b161-1fc196fe2e39-reasons-why-players-might-move">
<p>
The condition kept in mind while generating these: the reasoning must be expressed in terms already defined in the game structure, or derived from them.
</p>

<ul class="org-ul">
<li>The motivstates represent what players know about the outcomes - updating to add a property is adding a fact to the universe of consideration.</li>
<li>The motivstates represent "relevance" - updating to add a property necessitates dropping another one, and players optimize for reaching outcomes that would be preferred under a maximal motivstate that they can't actually ever hold.</li>
<li>The motivstates represent information about the game itself:
<ul class="org-ul">
<li>Reachability - it would be a simplification of the game tree</li>
<li>Whether some future game is gluable onto an outcome.</li>
<li>A fact about <i>the other players</i> (what type would that fact have?)
<ul class="org-ul">
<li>We are trying to assert that motivstates are things that players can infer about other players. What would they be trying to find out?
<ul class="org-ul">
<li>Easy answer: what the other players are going to do. So, a partial strategy? Something of the form "\(p \in M_{Alice}\) iff Alice expects Bob to move left at some point in future play".</li>
<li>Another easy answer: what players want. So: "\(f(p) \in M_{Alice}\) iff Alice thinks Bob is ranking some property \(p\) higher than all other properties."</li>
</ul></li>
</ul></li>
<li>Verbs - intentions that connect moves to ends, that can be inferred and matched to signalling moves over the course of play.</li>
</ul></li>
<li>The motivstates represent concerns that are apt to some environmental fact: e.g. the season dictating the parameters for selecting fruit. In monsoon you must look for thick skins, in summer you must look for high water content, etc. Interesting, because it's a way to show that the preference cycle might be entrained by an environmental cycle, and therefore rational in context.
<ul class="org-ul">
<li>This is called a zeitgeber. <a href="https://astralcodexten.substack.com/p/diseasonality?s=r">Diseasonality - by Scott Alexander - Astral Codex Ten</a> is where I first head about this; there's a LARGE body of literature attached to the idea of a zeitgeber, and I should read it if and when it seems like a good idea to. Modeling it using a similar kind of intransitivity to what we're playing with here would be pretty cool.
<ul class="org-ul">
<li>Spitball intuition: size of motivstate tracks how "tight" your curves can be on a cycle - shifting sands do not make for fine-grained preferences.
<ul class="org-ul">
<li>something something lotka-volterra
<ul class="org-ul">
<li>I do need to spend some time studying this magic when I can.</li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
</ul>
</div>
<div id="outline-container-d351c9aa-09b7-4129-b161-1fc196fe2e39-concrete-question-let-properties-be" class="outline-4">
<h4 id="d351c9aa-09b7-4129-b161-1fc196fe2e39-concrete-question-let-properties-be"><span class="section-number-4">3.2.1.</span> CONCRETE QUESTION: let properties be words in the builder-assistant game. How many rounds of play does it take before the epistemic game catches up to common knowledge of which properties constitute a player p's motivational state?</h4>
<div class="outline-text-4" id="text-d351c9aa-09b7-4129-b161-1fc196fe2e39-concrete-question-let-properties-be">
<p>
Let there exist a turn based two-player game.
</p>
<ul class="org-ul">
<li>We begin at state R
<ul class="org-ul">
<li>At R, A, and B, one can either</li>
<li>play a, which takes one to state A</li>
<li>play b, which takes one to state B</li>
<li>play s, which takes one to state S, where both players must play either x or y
<ul class="org-ul">
<li>if both players select x or both select y, we go to state WIN; each get one point.</li>
<li>else we go back to R.</li>
</ul></li>
</ul></li>
</ul>


<p>
This can be extended over some arbitrary alphabet of states-and-moves. Call that alphabet the builder-assistant language.
</p>

<p>
I think there exist property-based agents who have a winning strategy at this game.
</p>

<p>
Once we have the collaborative picture, then we can try to break it.
</p>


<ul class="org-ul">
<li>can we build a simulation in which siloing happens?
<ul class="org-ul">
<li>I'm defining "siloing" as "several coalitions achieving coordination internally takes fewer steps to reach than the grand coalition achieving coordination."</li>
</ul></li>
<li>can we define a complexity measure according to which a population speaking a language exhibits siloing, or other such effects, past a threshold in that measure?
<ul class="org-ul">
<li>My naive first guess for a complexity measure is "number of tokens in the motivstate."</li>
<li>My second guess is "minimum number of steps needed to achieve common knowledge of everybody's motivstate in a coalition."</li>
</ul></li>
<li>what is the "shape" of the game that exhibits the minimum number of steps needed to achieve common knowledge?</li>
</ul>



<p>
If I recall correctly you have mentioned you and Parkih 2004 as a referent for agents agreeing upon a protocol of further discourse. I will go read that this week.
</p>
</div>
</div>
</div>
<div class="outline-text-3" id="text-d351c9aa-09b7-4129-b161-1fc196fe2e39-reasons-why-players-might-move">
<p>
The condition kept in mind while generating these: the reasoning must be expressed in terms already defined in the game structure, or derived from them.
</p>

<ul class="org-ul">
<li>The motivstates represent what players know about the outcomes - updating to add a property is adding a fact to the universe of consideration.</li>
<li>The motivstates represent "relevance" - updating to add a property necessitates dropping another one, and players optimize for reaching outcomes that would be preferred under a maximal motivstate that they can't actually ever hold.</li>
<li>The motivstates represent information about the game itself:
<ul class="org-ul">
<li>Reachability - it would be a simplification of the game tree</li>
<li>Whether some future game is gluable onto an outcome.</li>
<li>A fact about <i>the other players</i> (what type would that fact have?)
<ul class="org-ul">
<li>We are trying to assert that motivstates are things that players can infer about other players. What would they be trying to find out?
<ul class="org-ul">
<li>Easy answer: what the other players are going to do. So, a partial strategy? Something of the form "\(p \in M_{Alice}\) iff Alice expects Bob to move left at some point in future play".</li>
<li>Another easy answer: what players want. So: "\(f(p) \in M_{Alice}\) iff Alice thinks Bob is ranking some property \(p\) higher than all other properties."</li>
</ul></li>
</ul></li>
<li>Verbs - intentions that connect moves to ends, that can be inferred and matched to signalling moves over the course of play.</li>
</ul></li>
<li>The motivstates represent concerns that are apt to some environmental fact: e.g. the season dictating the parameters for selecting fruit. In monsoon you must look for thick skins, in summer you must look for high water content, etc. Interesting, because it's a way to show that the preference cycle might be entrained by an environmental cycle, and therefore rational in context.
<ul class="org-ul">
<li>This is called a zeitgeber. <a href="https://astralcodexten.substack.com/p/diseasonality?s=r">Diseasonality - by Scott Alexander - Astral Codex Ten</a> is where I first head about this; there's a LARGE body of literature attached to the idea of a zeitgeber, and I should read it if and when it seems like a good idea to. Modeling it using a similar kind of intransitivity to what we're playing with here would be pretty cool.
<ul class="org-ul">
<li>Spitball intuition: size of motivstate tracks how "tight" your curves can be on a cycle - shifting sands do not make for fine-grained preferences.
<ul class="org-ul">
<li>something something lotka-volterra
<ul class="org-ul">
<li>I do need to spend some time studying this magic when I can.</li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
</ul>
</div>
<p>
The condition kept in mind while generating these: the reasoning must be expressed in terms already defined in the game structure, or derived from them.
</p>

<ul class="org-ul">
<li>The motivstates represent what players know about the outcomes - updating to add a property is adding a fact to the universe of consideration.</li>
<li>The motivstates represent "relevance" - updating to add a property necessitates dropping another one, and players optimize for reaching outcomes that would be preferred under a maximal motivstate that they can't actually ever hold.</li>
<li>The motivstates represent information about the game itself:
<ul class="org-ul">
<li>Reachability - it would be a simplification of the game tree</li>
<li>Whether some future game is gluable onto an outcome.</li>
<li>A fact about <i>the other players</i> (what type would that fact have?)
<ul class="org-ul">
<li>We are trying to assert that motivstates are things that players can infer about other players. What would they be trying to find out?
<ul class="org-ul">
<li>Easy answer: what the other players are going to do. So, a partial strategy? Something of the form "\(p \in M_{Alice}\) iff Alice expects Bob to move left at some point in future play".</li>
<li>Another easy answer: what players want. So: "\(f(p) \in M_{Alice}\) iff Alice thinks Bob is ranking some property \(p\) higher than all other properties."</li>
</ul></li>
</ul></li>
<li>Verbs - intentions that connect moves to ends, that can be inferred and matched to signalling moves over the course of play.</li>
</ul></li>
<li>The motivstates represent concerns that are apt to some environmental fact: e.g. the season dictating the parameters for selecting fruit. In monsoon you must look for thick skins, in summer you must look for high water content, etc. Interesting, because it's a way to show that the preference cycle might be entrained by an environmental cycle, and therefore rational in context.
<ul class="org-ul">
<li>This is called a zeitgeber. <a href="https://astralcodexten.substack.com/p/diseasonality?s=r">Diseasonality - by Scott Alexander - Astral Codex Ten</a> is where I first head about this; there's a LARGE body of literature attached to the idea of a zeitgeber, and I should read it if and when it seems like a good idea to. Modeling it using a similar kind of intransitivity to what we're playing with here would be pretty cool.
<ul class="org-ul">
<li>Spitball intuition: size of motivstate tracks how "tight" your curves can be on a cycle - shifting sands do not make for fine-grained preferences.
<ul class="org-ul">
<li>something something lotka-volterra
<ul class="org-ul">
<li>I do need to spend some time studying this magic when I can.</li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
</ul>

<li>The motivstates represent what players know about the outcomes - updating to add a property is adding a fact to the universe of consideration.</li>
The motivstates represent what players know about the outcomes - updating to add a property is adding a fact to the universe of consideration.
<li>The motivstates represent "relevance" - updating to add a property necessitates dropping another one, and players optimize for reaching outcomes that would be preferred under a maximal motivstate that they can't actually ever hold.</li>
The motivstates represent "relevance" - updating to add a property necessitates dropping another one, and players optimize for reaching outcomes that would be preferred under a maximal motivstate that they can't actually ever hold.
<li>The motivstates represent information about the game itself:
<ul class="org-ul">
<li>Reachability - it would be a simplification of the game tree</li>
<li>Whether some future game is gluable onto an outcome.</li>
<li>A fact about <i>the other players</i> (what type would that fact have?)
<ul class="org-ul">
<li>We are trying to assert that motivstates are things that players can infer about other players. What would they be trying to find out?
<ul class="org-ul">
<li>Easy answer: what the other players are going to do. So, a partial strategy? Something of the form "\(p \in M_{Alice}\) iff Alice expects Bob to move left at some point in future play".</li>
<li>Another easy answer: what players want. So: "\(f(p) \in M_{Alice}\) iff Alice thinks Bob is ranking some property \(p\) higher than all other properties."</li>
</ul></li>
</ul></li>
<li>Verbs - intentions that connect moves to ends, that can be inferred and matched to signalling moves over the course of play.</li>
</ul></li>
The motivstates represent information about the game itself:
<ul class="org-ul">
<li>Reachability - it would be a simplification of the game tree</li>
<li>Whether some future game is gluable onto an outcome.</li>
<li>A fact about <i>the other players</i> (what type would that fact have?)
<ul class="org-ul">
<li>We are trying to assert that motivstates are things that players can infer about other players. What would they be trying to find out?
<ul class="org-ul">
<li>Easy answer: what the other players are going to do. So, a partial strategy? Something of the form "\(p \in M_{Alice}\) iff Alice expects Bob to move left at some point in future play".</li>
<li>Another easy answer: what players want. So: "\(f(p) \in M_{Alice}\) iff Alice thinks Bob is ranking some property \(p\) higher than all other properties."</li>
</ul></li>
</ul></li>
<li>Verbs - intentions that connect moves to ends, that can be inferred and matched to signalling moves over the course of play.</li>
</ul>
<li>Reachability - it would be a simplification of the game tree</li>
Reachability - it would be a simplification of the game tree
<li>Whether some future game is gluable onto an outcome.</li>
Whether some future game is gluable onto an outcome.
<li>A fact about <i>the other players</i> (what type would that fact have?)
<ul class="org-ul">
<li>We are trying to assert that motivstates are things that players can infer about other players. What would they be trying to find out?
<ul class="org-ul">
<li>Easy answer: what the other players are going to do. So, a partial strategy? Something of the form "\(p \in M_{Alice}\) iff Alice expects Bob to move left at some point in future play".</li>
<li>Another easy answer: what players want. So: "\(f(p) \in M_{Alice}\) iff Alice thinks Bob is ranking some property \(p\) higher than all other properties."</li>
</ul></li>
</ul></li>
A fact about <i>the other players</i> (what type would that fact have?)
<ul class="org-ul">
<li>We are trying to assert that motivstates are things that players can infer about other players. What would they be trying to find out?
<ul class="org-ul">
<li>Easy answer: what the other players are going to do. So, a partial strategy? Something of the form "\(p \in M_{Alice}\) iff Alice expects Bob to move left at some point in future play".</li>
<li>Another easy answer: what players want. So: "\(f(p) \in M_{Alice}\) iff Alice thinks Bob is ranking some property \(p\) higher than all other properties."</li>
</ul></li>
</ul>
<li>We are trying to assert that motivstates are things that players can infer about other players. What would they be trying to find out?
<ul class="org-ul">
<li>Easy answer: what the other players are going to do. So, a partial strategy? Something of the form "\(p \in M_{Alice}\) iff Alice expects Bob to move left at some point in future play".</li>
<li>Another easy answer: what players want. So: "\(f(p) \in M_{Alice}\) iff Alice thinks Bob is ranking some property \(p\) higher than all other properties."</li>
</ul></li>
We are trying to assert that motivstates are things that players can infer about other players. What would they be trying to find out?
<ul class="org-ul">
<li>Easy answer: what the other players are going to do. So, a partial strategy? Something of the form "\(p \in M_{Alice}\) iff Alice expects Bob to move left at some point in future play".</li>
<li>Another easy answer: what players want. So: "\(f(p) \in M_{Alice}\) iff Alice thinks Bob is ranking some property \(p\) higher than all other properties."</li>
</ul>
<li>Easy answer: what the other players are going to do. So, a partial strategy? Something of the form "\(p \in M_{Alice}\) iff Alice expects Bob to move left at some point in future play".</li>
Easy answer: what the other players are going to do. So, a partial strategy? Something of the form "\(p \in M_{Alice}\) iff Alice expects Bob to move left at some point in future play".
<li>Another easy answer: what players want. So: "\(f(p) \in M_{Alice}\) iff Alice thinks Bob is ranking some property \(p\) higher than all other properties."</li>
Another easy answer: what players want. So: "\(f(p) \in M_{Alice}\) iff Alice thinks Bob is ranking some property \(p\) higher than all other properties."
<li>Verbs - intentions that connect moves to ends, that can be inferred and matched to signalling moves over the course of play.</li>
Verbs - intentions that connect moves to ends, that can be inferred and matched to signalling moves over the course of play.
<li>The motivstates represent concerns that are apt to some environmental fact: e.g. the season dictating the parameters for selecting fruit. In monsoon you must look for thick skins, in summer you must look for high water content, etc. Interesting, because it's a way to show that the preference cycle might be entrained by an environmental cycle, and therefore rational in context.
<ul class="org-ul">
<li>This is called a zeitgeber. <a href="https://astralcodexten.substack.com/p/diseasonality?s=r">Diseasonality - by Scott Alexander - Astral Codex Ten</a> is where I first head about this; there's a LARGE body of literature attached to the idea of a zeitgeber, and I should read it if and when it seems like a good idea to. Modeling it using a similar kind of intransitivity to what we're playing with here would be pretty cool.
<ul class="org-ul">
<li>Spitball intuition: size of motivstate tracks how "tight" your curves can be on a cycle - shifting sands do not make for fine-grained preferences.
<ul class="org-ul">
<li>something something lotka-volterra
<ul class="org-ul">
<li>I do need to spend some time studying this magic when I can.</li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
The motivstates represent concerns that are apt to some environmental fact: e.g. the season dictating the parameters for selecting fruit. In monsoon you must look for thick skins, in summer you must look for high water content, etc. Interesting, because it's a way to show that the preference cycle might be entrained by an environmental cycle, and therefore rational in context.
<ul class="org-ul">
<li>This is called a zeitgeber. <a href="https://astralcodexten.substack.com/p/diseasonality?s=r">Diseasonality - by Scott Alexander - Astral Codex Ten</a> is where I first head about this; there's a LARGE body of literature attached to the idea of a zeitgeber, and I should read it if and when it seems like a good idea to. Modeling it using a similar kind of intransitivity to what we're playing with here would be pretty cool.
<ul class="org-ul">
<li>Spitball intuition: size of motivstate tracks how "tight" your curves can be on a cycle - shifting sands do not make for fine-grained preferences.
<ul class="org-ul">
<li>something something lotka-volterra
<ul class="org-ul">
<li>I do need to spend some time studying this magic when I can.</li>
</ul></li>
</ul></li>
</ul></li>
</ul>
<li>This is called a zeitgeber. <a href="https://astralcodexten.substack.com/p/diseasonality?s=r">Diseasonality - by Scott Alexander - Astral Codex Ten</a> is where I first head about this; there's a LARGE body of literature attached to the idea of a zeitgeber, and I should read it if and when it seems like a good idea to. Modeling it using a similar kind of intransitivity to what we're playing with here would be pretty cool.
<ul class="org-ul">
<li>Spitball intuition: size of motivstate tracks how "tight" your curves can be on a cycle - shifting sands do not make for fine-grained preferences.
<ul class="org-ul">
<li>something something lotka-volterra
<ul class="org-ul">
<li>I do need to spend some time studying this magic when I can.</li>
</ul></li>
</ul></li>
</ul></li>
This is called a zeitgeber. <a href="https://astralcodexten.substack.com/p/diseasonality?s=r">Diseasonality - by Scott Alexander - Astral Codex Ten</a> is where I first head about this; there's a LARGE body of literature attached to the idea of a zeitgeber, and I should read it if and when it seems like a good idea to. Modeling it using a similar kind of intransitivity to what we're playing with here would be pretty cool.
<ul class="org-ul">
<li>Spitball intuition: size of motivstate tracks how "tight" your curves can be on a cycle - shifting sands do not make for fine-grained preferences.
<ul class="org-ul">
<li>something something lotka-volterra
<ul class="org-ul">
<li>I do need to spend some time studying this magic when I can.</li>
</ul></li>
</ul></li>
</ul>
<li>Spitball intuition: size of motivstate tracks how "tight" your curves can be on a cycle - shifting sands do not make for fine-grained preferences.
<ul class="org-ul">
<li>something something lotka-volterra
<ul class="org-ul">
<li>I do need to spend some time studying this magic when I can.</li>
</ul></li>
</ul></li>
Spitball intuition: size of motivstate tracks how "tight" your curves can be on a cycle - shifting sands do not make for fine-grained preferences.
<ul class="org-ul">
<li>something something lotka-volterra
<ul class="org-ul">
<li>I do need to spend some time studying this magic when I can.</li>
</ul></li>
</ul>
<li>something something lotka-volterra
<ul class="org-ul">
<li>I do need to spend some time studying this magic when I can.</li>
</ul></li>
something something lotka-volterra
<ul class="org-ul">
<li>I do need to spend some time studying this magic when I can.</li>
</ul>
<li>I do need to spend some time studying this magic when I can.</li>
I do need to spend some time studying this magic when I can.
<div id="outline-container-d351c9aa-09b7-4129-b161-1fc196fe2e39-concrete-question-let-properties-be" class="outline-4">
<h4 id="d351c9aa-09b7-4129-b161-1fc196fe2e39-concrete-question-let-properties-be"><span class="section-number-4">3.2.1.</span> CONCRETE QUESTION: let properties be words in the builder-assistant game. How many rounds of play does it take before the epistemic game catches up to common knowledge of which properties constitute a player p's motivational state?</h4>
<div class="outline-text-4" id="text-d351c9aa-09b7-4129-b161-1fc196fe2e39-concrete-question-let-properties-be">
<p>
Let there exist a turn based two-player game.
</p>
<ul class="org-ul">
<li>We begin at state R
<ul class="org-ul">
<li>At R, A, and B, one can either</li>
<li>play a, which takes one to state A</li>
<li>play b, which takes one to state B</li>
<li>play s, which takes one to state S, where both players must play either x or y
<ul class="org-ul">
<li>if both players select x or both select y, we go to state WIN; each get one point.</li>
<li>else we go back to R.</li>
</ul></li>
</ul></li>
</ul>


<p>
This can be extended over some arbitrary alphabet of states-and-moves. Call that alphabet the builder-assistant language.
</p>

<p>
I think there exist property-based agents who have a winning strategy at this game.
</p>

<p>
Once we have the collaborative picture, then we can try to break it.
</p>


<ul class="org-ul">
<li>can we build a simulation in which siloing happens?
<ul class="org-ul">
<li>I'm defining "siloing" as "several coalitions achieving coordination internally takes fewer steps to reach than the grand coalition achieving coordination."</li>
</ul></li>
<li>can we define a complexity measure according to which a population speaking a language exhibits siloing, or other such effects, past a threshold in that measure?
<ul class="org-ul">
<li>My naive first guess for a complexity measure is "number of tokens in the motivstate."</li>
<li>My second guess is "minimum number of steps needed to achieve common knowledge of everybody's motivstate in a coalition."</li>
</ul></li>
<li>what is the "shape" of the game that exhibits the minimum number of steps needed to achieve common knowledge?</li>
</ul>



<p>
If I recall correctly you have mentioned you and Parkih 2004 as a referent for agents agreeing upon a protocol of further discourse. I will go read that this week.
</p>
</div>
</div>
<div class="outline-text-4" id="text-d351c9aa-09b7-4129-b161-1fc196fe2e39-concrete-question-let-properties-be">
<p>
Let there exist a turn based two-player game.
</p>
<ul class="org-ul">
<li>We begin at state R
<ul class="org-ul">
<li>At R, A, and B, one can either</li>
<li>play a, which takes one to state A</li>
<li>play b, which takes one to state B</li>
<li>play s, which takes one to state S, where both players must play either x or y
<ul class="org-ul">
<li>if both players select x or both select y, we go to state WIN; each get one point.</li>
<li>else we go back to R.</li>
</ul></li>
</ul></li>
</ul>


<p>
This can be extended over some arbitrary alphabet of states-and-moves. Call that alphabet the builder-assistant language.
</p>

<p>
I think there exist property-based agents who have a winning strategy at this game.
</p>

<p>
Once we have the collaborative picture, then we can try to break it.
</p>


<ul class="org-ul">
<li>can we build a simulation in which siloing happens?
<ul class="org-ul">
<li>I'm defining "siloing" as "several coalitions achieving coordination internally takes fewer steps to reach than the grand coalition achieving coordination."</li>
</ul></li>
<li>can we define a complexity measure according to which a population speaking a language exhibits siloing, or other such effects, past a threshold in that measure?
<ul class="org-ul">
<li>My naive first guess for a complexity measure is "number of tokens in the motivstate."</li>
<li>My second guess is "minimum number of steps needed to achieve common knowledge of everybody's motivstate in a coalition."</li>
</ul></li>
<li>what is the "shape" of the game that exhibits the minimum number of steps needed to achieve common knowledge?</li>
</ul>



<p>
If I recall correctly you have mentioned you and Parkih 2004 as a referent for agents agreeing upon a protocol of further discourse. I will go read that this week.
</p>
</div>
<p>
Let there exist a turn based two-player game.
</p>
<ul class="org-ul">
<li>We begin at state R
<ul class="org-ul">
<li>At R, A, and B, one can either</li>
<li>play a, which takes one to state A</li>
<li>play b, which takes one to state B</li>
<li>play s, which takes one to state S, where both players must play either x or y
<ul class="org-ul">
<li>if both players select x or both select y, we go to state WIN; each get one point.</li>
<li>else we go back to R.</li>
</ul></li>
</ul></li>
</ul>


<li>We begin at state R
<ul class="org-ul">
<li>At R, A, and B, one can either</li>
<li>play a, which takes one to state A</li>
<li>play b, which takes one to state B</li>
<li>play s, which takes one to state S, where both players must play either x or y
<ul class="org-ul">
<li>if both players select x or both select y, we go to state WIN; each get one point.</li>
<li>else we go back to R.</li>
</ul></li>
</ul></li>
We begin at state R
<ul class="org-ul">
<li>At R, A, and B, one can either</li>
<li>play a, which takes one to state A</li>
<li>play b, which takes one to state B</li>
<li>play s, which takes one to state S, where both players must play either x or y
<ul class="org-ul">
<li>if both players select x or both select y, we go to state WIN; each get one point.</li>
<li>else we go back to R.</li>
</ul></li>
</ul>
<li>At R, A, and B, one can either</li>
At R, A, and B, one can either
<li>play a, which takes one to state A</li>
play a, which takes one to state A
<li>play b, which takes one to state B</li>
play b, which takes one to state B
<li>play s, which takes one to state S, where both players must play either x or y
<ul class="org-ul">
<li>if both players select x or both select y, we go to state WIN; each get one point.</li>
<li>else we go back to R.</li>
</ul></li>
play s, which takes one to state S, where both players must play either x or y
<ul class="org-ul">
<li>if both players select x or both select y, we go to state WIN; each get one point.</li>
<li>else we go back to R.</li>
</ul>
<li>if both players select x or both select y, we go to state WIN; each get one point.</li>
if both players select x or both select y, we go to state WIN; each get one point.
<li>else we go back to R.</li>
else we go back to R.
<p>
This can be extended over some arbitrary alphabet of states-and-moves. Call that alphabet the builder-assistant language.
</p>

<p>
I think there exist property-based agents who have a winning strategy at this game.
</p>

<p>
Once we have the collaborative picture, then we can try to break it.
</p>


<ul class="org-ul">
<li>can we build a simulation in which siloing happens?
<ul class="org-ul">
<li>I'm defining "siloing" as "several coalitions achieving coordination internally takes fewer steps to reach than the grand coalition achieving coordination."</li>
</ul></li>
<li>can we define a complexity measure according to which a population speaking a language exhibits siloing, or other such effects, past a threshold in that measure?
<ul class="org-ul">
<li>My naive first guess for a complexity measure is "number of tokens in the motivstate."</li>
<li>My second guess is "minimum number of steps needed to achieve common knowledge of everybody's motivstate in a coalition."</li>
</ul></li>
<li>what is the "shape" of the game that exhibits the minimum number of steps needed to achieve common knowledge?</li>
</ul>



<li>can we build a simulation in which siloing happens?
<ul class="org-ul">
<li>I'm defining "siloing" as "several coalitions achieving coordination internally takes fewer steps to reach than the grand coalition achieving coordination."</li>
</ul></li>
can we build a simulation in which siloing happens?
<ul class="org-ul">
<li>I'm defining "siloing" as "several coalitions achieving coordination internally takes fewer steps to reach than the grand coalition achieving coordination."</li>
</ul>
<li>I'm defining "siloing" as "several coalitions achieving coordination internally takes fewer steps to reach than the grand coalition achieving coordination."</li>
I'm defining "siloing" as "several coalitions achieving coordination internally takes fewer steps to reach than the grand coalition achieving coordination."
<li>can we define a complexity measure according to which a population speaking a language exhibits siloing, or other such effects, past a threshold in that measure?
<ul class="org-ul">
<li>My naive first guess for a complexity measure is "number of tokens in the motivstate."</li>
<li>My second guess is "minimum number of steps needed to achieve common knowledge of everybody's motivstate in a coalition."</li>
</ul></li>
can we define a complexity measure according to which a population speaking a language exhibits siloing, or other such effects, past a threshold in that measure?
<ul class="org-ul">
<li>My naive first guess for a complexity measure is "number of tokens in the motivstate."</li>
<li>My second guess is "minimum number of steps needed to achieve common knowledge of everybody's motivstate in a coalition."</li>
</ul>
<li>My naive first guess for a complexity measure is "number of tokens in the motivstate."</li>
My naive first guess for a complexity measure is "number of tokens in the motivstate."
<li>My second guess is "minimum number of steps needed to achieve common knowledge of everybody's motivstate in a coalition."</li>
My second guess is "minimum number of steps needed to achieve common knowledge of everybody's motivstate in a coalition."
<li>what is the "shape" of the game that exhibits the minimum number of steps needed to achieve common knowledge?</li>
what is the "shape" of the game that exhibits the minimum number of steps needed to achieve common knowledge?
<p>
If I recall correctly you have mentioned you and Parkih 2004 as a referent for agents agreeing upon a protocol of further discourse. I will go read that this week.
</p>


<div id="outline-container-d351c9aa-09b7-4129-b161-1fc196fe2e39-the-reason-dl-have-presented" class="outline-2">
<h2 id="d351c9aa-09b7-4129-b161-1fc196fe2e39-the-reason-dl-have-presented"><span class="section-number-2">4.</span> the reason DL have presented an axiomatization is that axioms are falsifiable statements.</h2>
<div class="outline-text-2" id="text-d351c9aa-09b7-4129-b161-1fc196fe2e39-the-reason-dl-have-presented">
<p>
They serve as the condition A in the guarded statement "if A holds in universe U then model M holds in universe U".
</p>

<p>
We can check if A holds per falisfiability, and elevate the rest of model M to hold also if it does.
</p>

<p>
SO: I ought to find a dataset of preferences to test for the axioms suggested. I will go hunting. Time to strengthen our semiotics.
</p>


<p>
This has been last week and part of the ten days preceding, adapted from my notes, the taking of which I've reapplied myself to. I hope I'm on a useful track (or at least a few useful tracks out of the many I seem to be on - convergence seems nigh, but I can't be sure.) Let me know what you think, whether here or in call. Speaking of: is this Friday good for you or would an alternate time be better?
</p>


<p>
See you soon, and I hope to find you well.
</p>

<p>
Sahiti
</p>
</div>
</div>
<div class="outline-text-2" id="text-d351c9aa-09b7-4129-b161-1fc196fe2e39-the-reason-dl-have-presented">
<p>
They serve as the condition A in the guarded statement "if A holds in universe U then model M holds in universe U".
</p>

<p>
We can check if A holds per falisfiability, and elevate the rest of model M to hold also if it does.
</p>

<p>
SO: I ought to find a dataset of preferences to test for the axioms suggested. I will go hunting. Time to strengthen our semiotics.
</p>


<p>
This has been last week and part of the ten days preceding, adapted from my notes, the taking of which I've reapplied myself to. I hope I'm on a useful track (or at least a few useful tracks out of the many I seem to be on - convergence seems nigh, but I can't be sure.) Let me know what you think, whether here or in call. Speaking of: is this Friday good for you or would an alternate time be better?
</p>


<p>
See you soon, and I hope to find you well.
</p>

<p>
Sahiti
</p>
</div>
<p>
They serve as the condition A in the guarded statement "if A holds in universe U then model M holds in universe U".
</p>

<p>
We can check if A holds per falisfiability, and elevate the rest of model M to hold also if it does.
</p>

<p>
SO: I ought to find a dataset of preferences to test for the axioms suggested. I will go hunting. Time to strengthen our semiotics.
</p>


<p>
This has been last week and part of the ten days preceding, adapted from my notes, the taking of which I've reapplied myself to. I hope I'm on a useful track (or at least a few useful tracks out of the many I seem to be on - convergence seems nigh, but I can't be sure.) Let me know what you think, whether here or in call. Speaking of: is this Friday good for you or would an alternate time be better?
</p>


<p>
See you soon, and I hope to find you well.
</p>

<p>
Sahiti
</p>
<div id="outline-container-d351c9aa-09b7-4129-b161-1fc196fe2e39-backlinks" class="outline-2">
<h2 id="d351c9aa-09b7-4129-b161-1fc196fe2e39-backlinks"><span class="section-number-2">5.</span> Backlinks</h2>
<div class="outline-text-2" id="text-d351c9aa-09b7-4129-b161-1fc196fe2e39-backlinks">
</div>
<ul class="org-ul">
</ul>
</div>
<ul class="org-ul">
</ul>
</div>
<div id="postamble" class="status">
<p class="author">Author: Nix build user</p>
<p class="validation"><a href="https://validator.w3.org/check?uri=referer">Validate</a></p>
</div>
</body>
</html>
